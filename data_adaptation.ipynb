{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc33805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path=\"pq_corpus.txt\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Example usage\n",
    "extract_text_from_pdf(\"pq_textbook.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb327954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmathew2020\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rmathew2020\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 26838 examples [00:00, 797084.97 examples/s]\n",
      "Map: 100%|██████████| 26838/26838 [00:02<00:00, 11370.21 examples/s]\n",
      " 10%|▉         | 500/5034 [30:14<5:32:29,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1743, 'grad_norm': 22.645418167114258, 'learning_rate': 4.503377036154152e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1000/5034 [59:01<3:56:10,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8826, 'grad_norm': 18.324779510498047, 'learning_rate': 4.0067540723083036e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1500/5034 [1:27:46<3:25:50,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7576, 'grad_norm': 14.610793113708496, 'learning_rate': 3.5101311084624553e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2000/5034 [1:56:04<2:50:12,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6183, 'grad_norm': 30.54942512512207, 'learning_rate': 3.0135081446166074e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 2500/5034 [2:23:42<2:09:51,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.578, 'grad_norm': 17.067272186279297, 'learning_rate': 2.5168851807707588e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 3000/5034 [2:49:40<1:52:01,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5107, 'grad_norm': 13.001250267028809, 'learning_rate': 2.0202622169249108e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 3500/5034 [3:16:11<1:23:24,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4417, 'grad_norm': 19.580732345581055, 'learning_rate': 1.5236392530790625e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4000/5034 [3:46:53<1:18:16,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2824, 'grad_norm': 18.23748207092285, 'learning_rate': 1.0270162892332142e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 4500/5034 [4:13:29<26:22,  2.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2569, 'grad_norm': 16.32189178466797, 'learning_rate': 5.30393325387366e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 5000/5034 [4:38:24<01:44,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2809, 'grad_norm': 18.75294303894043, 'learning_rate': 3.3770361541517685e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5034/5034 [4:40:05<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16805.8714, 'train_samples_per_second': 4.791, 'train_steps_per_second': 0.3, 'train_loss': 2.577739136627282, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Load the tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load the plain text dataset\n",
    "datasets = load_dataset('text', data_files={'train': 'pq_corpus.txt'})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Set up data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-pq-mlm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "trainer.train()\n",
    "\n",
    "model_save_path = \"./distilbert-pq-mlm\"\n",
    "\n",
    "# Save the domain-adapted model\n",
    "trainer.save_model(\"./distilbert-pq-mlm\")\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Verify saved files\n",
    "print(f\"Model and tokenizer saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5fd06",
   "metadata": {},
   "source": [
    "281 mins 40.6 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956e675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing domain-adapted model:\n",
      "--------------------------------------------------\n",
      "\n",
      "Input: A sudden [MASK] in voltage can cause sensitive equipment to fail.\n",
      "Top predictions:\n",
      "- increase: 0.4798\n",
      "- drop: 0.1674\n",
      "- rise: 0.1308\n",
      "- change: 0.0904\n",
      "- surge: 0.0441\n",
      "\n",
      "Input: The power [MASK] was caused by lightning strike.\n",
      "Top predictions:\n",
      "- problem: 0.1721\n",
      "- loss: 0.1222\n",
      "- quality: 0.0741\n",
      "- interruption: 0.0732\n",
      "- system: 0.0654\n",
      "\n",
      "Input: The distribution [MASK] failed due to overload.\n",
      "Top predictions:\n",
      "- system: 0.5795\n",
      "- feeder: 0.1961\n",
      "- line: 0.0223\n",
      "- systems: 0.0182\n",
      "- lines: 0.0142\n",
      "\n",
      "Input: Multiple [MASK] faults occurred during the thunderstorm.\n",
      "Top predictions:\n",
      "- ground: 0.1447\n",
      "- surge: 0.0926\n",
      "- lightning: 0.0635\n",
      "- fault: 0.0274\n",
      "- transient: 0.0264\n",
      "\n",
      "Input: The [MASK] waveform showed significant distortion.\n",
      "Top predictions:\n",
      "- first: 0.1249\n",
      "- resulting: 0.0728\n",
      "- initial: 0.0675\n",
      "- input: 0.0616\n",
      "- original: 0.0550\n",
      "\n",
      "Input: [MASK] fell on power lines during a storm, affecting multiple feeders; estimated restoration time is 4 hours.\n",
      "Top predictions:\n",
      "- that: 0.1861\n",
      "- they: 0.0442\n",
      "- cables: 0.0318\n",
      "- it: 0.0285\n",
      "- lightning: 0.0214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test loading\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "test_model = AutoModelForMaskedLM.from_pretrained(model_save_path)\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=test_model, tokenizer=test_tokenizer)\n",
    "\n",
    "# Example test sentences relevant to power quality\n",
    "test_sentences = [\n",
    "    \"A sudden [MASK] in voltage can cause sensitive equipment to fail.\",\n",
    "    \"The power [MASK] was caused by lightning strike.\",\n",
    "    \"The distribution [MASK] failed due to overload.\",\n",
    "    \"Multiple [MASK] faults occurred during the thunderstorm.\",\n",
    "    \"The [MASK] waveform showed significant distortion.\", \n",
    "    \"[MASK] fell on power lines during a storm, affecting multiple feeders; estimated restoration time is 4 hours.\"\n",
    "]\n",
    "\n",
    "# Run predictions\n",
    "print(\"\\nTesting domain-adapted model:\")\n",
    "print(\"-\" * 50)\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nInput: {sentence}\")\n",
    "    results = fill_mask(sentence)\n",
    "    print(\"Top predictions:\")\n",
    "    for r in results[:5]:  # Show top 5 predictions\n",
    "        print(f\"- {r['token_str']}: {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32d463",
   "metadata": {},
   "source": [
    "# more textbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0d155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 16293 examples [00:00, 766522.66 examples/s]\n",
      "Map: 100%|██████████| 16293/16293 [00:01<00:00, 11451.57 examples/s]\n",
      " 16%|█▋        | 500/3057 [26:45<2:40:21,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2649, 'grad_norm': 23.426637649536133, 'learning_rate': 4.182204775924109e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1000/3057 [53:36<1:41:39,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1102, 'grad_norm': 19.356590270996094, 'learning_rate': 3.3644095518482174e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 1500/3057 [1:19:02<1:16:48,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.856, 'grad_norm': 21.036420822143555, 'learning_rate': 2.546614327772326e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 2000/3057 [1:44:19<53:04,  3.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7942, 'grad_norm': 18.880910873413086, 'learning_rate': 1.7288191036964345e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2500/3057 [2:09:48<27:50,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6668, 'grad_norm': 16.75374984741211, 'learning_rate': 9.110238796205431e-06, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 3000/3057 [2:34:38<02:47,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5409, 'grad_norm': 17.763526916503906, 'learning_rate': 9.322865554465163e-07, 'epoch': 2.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3057/3057 [2:37:25<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9445.6973, 'train_samples_per_second': 5.175, 'train_steps_per_second': 0.324, 'train_loss': 2.866854726624793, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./distilbert-pq-enhanced\\\\tokenizer_config.json',\n",
       " './distilbert-pq-enhanced\\\\special_tokens_map.json',\n",
       " './distilbert-pq-enhanced\\\\vocab.txt',\n",
       " './distilbert-pq-enhanced\\\\added_tokens.json',\n",
       " './distilbert-pq-enhanced\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "\n",
    "# Load your previously fine-tuned model and its tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./distilbert-pq-mlm\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"./distilbert-pq-mlm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Extract text from a new textbook\n",
    "extract_text_from_pdf(\"t2.pdf\", output_txt_path=\"new_corpus.txt\")\n",
    "\n",
    "# Load the text from the new textbook\n",
    "datasets = load_dataset('text', data_files={'train': 'new_corpus.txt'})\n",
    "\n",
    "# The rest of your tokenization and training code remains the same\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# Define training arguments for continued training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-pq-enhanced\",  # New path for the enhanced model\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer with your previously fine-tuned model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Continue training\n",
    "trainer.train()\n",
    "\n",
    "# Save the further enhanced model\n",
    "model_save_path = \"./distilbert-pq-enhanced\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292d4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing domain-adapted model:\n",
      "--------------------------------------------------\n",
      "\n",
      "Input: Customer reported flickering lights and a [MASK] smell near the panel.\n",
      "Top predictions:\n",
      "- foul: 0.2724\n",
      "- metallic: 0.0471\n",
      "- toxic: 0.0444\n",
      "- rank: 0.0369\n",
      "- burning: 0.0340\n",
      "\n",
      "Input: A [MASK] was discovered inside the transformer cabinet, likely causing the outage.\n",
      "Top predictions:\n",
      "- leak: 0.3965\n",
      "- fault: 0.1289\n",
      "- fuse: 0.1183\n",
      "- spark: 0.0247\n",
      "- conductor: 0.0180\n",
      "\n",
      "Input: SCADA alarm triggered for [MASK] voltage detected at substation 4C.\n",
      "Top predictions:\n",
      "- high: 0.2192\n",
      "- the: 0.1270\n",
      "- excessive: 0.0916\n",
      "- any: 0.0582\n",
      "- fault: 0.0311\n",
      "\n",
      "Input: Feeder F12 tripped due to an [MASK] on phase B during high winds.\n",
      "Top predictions:\n",
      "- arc: 0.7042\n",
      "- interruption: 0.0848\n",
      "- fault: 0.0404\n",
      "- impact: 0.0337\n",
      "- accident: 0.0249\n",
      "\n",
      "Input: Maintenance crew removed a [MASK] nest from the capacitor bank enclosure.\n",
      "Top predictions:\n",
      "- squirrel: 0.1528\n",
      "- tree: 0.0730\n",
      "- slot: 0.0652\n",
      "- seal: 0.0528\n",
      "- tape: 0.0252\n",
      "\n",
      "Input: An [MASK] caused a line-to-ground fault near pole 36.\n",
      "Top predictions:\n",
      "- arc: 0.4893\n",
      "- ##ce: 0.1747\n",
      "- accident: 0.0660\n",
      "- interruption: 0.0536\n",
      "- event: 0.0179\n",
      "\n",
      "Input: The service transformer experienced [MASK] oil leakage.\n",
      "Top predictions:\n",
      "- an: 0.6013\n",
      "- some: 0.0201\n",
      "- severe: 0.0191\n",
      "- heavy: 0.0176\n",
      "- considerable: 0.0153\n",
      "\n",
      "Input: Dispatch noted a [MASK] arc near the insulator on structure 112.\n",
      "Top predictions:\n",
      "- small: 0.0668\n",
      "- direct: 0.0426\n",
      "- loose: 0.0405\n",
      "- voltage: 0.0378\n",
      "- brief: 0.0372\n",
      "\n",
      "Input: Post-storm inspection found a [MASK] hanging on the primary conductor.\n",
      "Top predictions:\n",
      "- cable: 0.0718\n",
      "- tape: 0.0694\n",
      "- fuse: 0.0689\n",
      "- seal: 0.0610\n",
      "- tree: 0.0549\n",
      "\n",
      "Input: Customer reported [MASK] noise from the overhead lines during peak load.\n",
      "Top predictions:\n",
      "- the: 0.1220\n",
      "- any: 0.0986\n",
      "- excessive: 0.0527\n",
      "- that: 0.0380\n",
      "- no: 0.0295\n",
      "\n",
      "Input: The [MASK] tripped multiple times overnight, suspect loose connector.\n",
      "Top predictions:\n",
      "- motor: 0.1883\n",
      "- machine: 0.1083\n",
      "- breaker: 0.0489\n",
      "- fuse: 0.0444\n",
      "- cable: 0.0416\n",
      "\n",
      "Input: Protective relay flagged a [MASK] imbalance in feeder 27A.\n",
      "Top predictions:\n",
      "- voltage: 0.7990\n",
      "- temperature: 0.0615\n",
      "- current: 0.0216\n",
      "- harmonic: 0.0155\n",
      "- speed: 0.0120\n",
      "\n",
      "Input: [MASK] identified on DGA report—suggest potential internal fault.\n",
      "Top predictions:\n",
      "- are: 0.1253\n",
      "- as: 0.0949\n",
      "- is: 0.0711\n",
      "- be: 0.0691\n",
      "- not: 0.0344\n",
      "\n",
      "Input: Crews isolated section B for suspected [MASK] in underground cable.\n",
      "Top predictions:\n",
      "- faults: 0.8586\n",
      "- failures: 0.0155\n",
      "- problems: 0.0119\n",
      "- fault: 0.0098\n",
      "- connections: 0.0075\n",
      "\n",
      "Input: Outage caused by [MASK] falling across all three phases near intersection.\n",
      "Top predictions:\n",
      "- cables: 0.0812\n",
      "- wires: 0.0743\n",
      "- lightning: 0.0738\n",
      "- trees: 0.0735\n",
      "- conductors: 0.0517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test loading\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "test_model = AutoModelForMaskedLM.from_pretrained(model_save_path)\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=test_model, tokenizer=test_tokenizer)\n",
    "\n",
    "# Example test sentences relevant to power quality\n",
    "test_sentences = [\n",
    "    \"A sudden [MASK] in voltage can cause sensitive equipment to fail.\",\n",
    "    \"The power [MASK] was caused by lightning strike.\",\n",
    "    \"The distribution [MASK] failed due to overload.\",\n",
    "    \"Multiple [MASK] faults occurred during the thunderstorm.\",\n",
    "    \"The [MASK] waveform showed significant distortion.\", \n",
    "    \"[MASK] fell on power lines during a storm, affecting multiple feeders; estimated restoration time is 4 hours.\"\n",
    "]\n",
    "\n",
    "test_sentences2 = [\n",
    "    \"Customer reported flickering lights and a [MASK] smell near the panel.\",\n",
    "    \"A [MASK] was discovered inside the transformer cabinet, likely causing the outage.\",\n",
    "    \"SCADA alarm triggered for [MASK] voltage detected at substation 4C.\",\n",
    "    \"Feeder F12 tripped due to an [MASK] on phase B during high winds.\",\n",
    "    \"Maintenance crew removed a [MASK] nest from the capacitor bank enclosure.\",\n",
    "    \"An [MASK] caused a line-to-ground fault near pole 36.\",\n",
    "    \"The service transformer experienced [MASK] oil leakage.\",\n",
    "    \"Dispatch noted a [MASK] arc near the insulator on structure 112.\",\n",
    "    \"Post-storm inspection found a [MASK] hanging on the primary conductor.\",\n",
    "    \"Customer reported [MASK] noise from the overhead lines during peak load.\",\n",
    "    \"The [MASK] tripped multiple times overnight, suspect loose connector.\",\n",
    "    \"Protective relay flagged a [MASK] imbalance in feeder 27A.\",\n",
    "    \"[MASK] identified on DGA report—suggest potential internal fault.\",\n",
    "    \"Crews isolated section B for suspected [MASK] in underground cable.\",\n",
    "    \"Outage caused by [MASK] falling across all three phases near intersection.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Run predictions\n",
    "print(\"\\nTesting domain-adapted model:\")\n",
    "print(\"-\" * 50)\n",
    "for sentence in test_sentences2:\n",
    "    print(f\"\\nInput: {sentence}\")\n",
    "    results = fill_mask(sentence)\n",
    "    print(\"Top predictions:\")\n",
    "    for r in results[:5]:  # Show top 5 predictions\n",
    "        print(f\"- {r['token_str']}: {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96801965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
