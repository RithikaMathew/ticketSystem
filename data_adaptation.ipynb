{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc33805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path=\"pq_corpus.txt\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Example usage\n",
    "extract_text_from_pdf(\"pq_textbook.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb327954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmathew2020\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rmathew2020\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 26838 examples [00:00, 797084.97 examples/s]\n",
      "Map: 100%|██████████| 26838/26838 [00:02<00:00, 11370.21 examples/s]\n",
      " 10%|▉         | 500/5034 [30:14<5:32:29,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1743, 'grad_norm': 22.645418167114258, 'learning_rate': 4.503377036154152e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1000/5034 [59:01<3:56:10,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8826, 'grad_norm': 18.324779510498047, 'learning_rate': 4.0067540723083036e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1500/5034 [1:27:46<3:25:50,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7576, 'grad_norm': 14.610793113708496, 'learning_rate': 3.5101311084624553e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2000/5034 [1:56:04<2:50:12,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6183, 'grad_norm': 30.54942512512207, 'learning_rate': 3.0135081446166074e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 2500/5034 [2:23:42<2:09:51,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.578, 'grad_norm': 17.067272186279297, 'learning_rate': 2.5168851807707588e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 3000/5034 [2:49:40<1:52:01,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5107, 'grad_norm': 13.001250267028809, 'learning_rate': 2.0202622169249108e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 3500/5034 [3:16:11<1:23:24,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4417, 'grad_norm': 19.580732345581055, 'learning_rate': 1.5236392530790625e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4000/5034 [3:46:53<1:18:16,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2824, 'grad_norm': 18.23748207092285, 'learning_rate': 1.0270162892332142e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 4500/5034 [4:13:29<26:22,  2.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2569, 'grad_norm': 16.32189178466797, 'learning_rate': 5.30393325387366e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 5000/5034 [4:38:24<01:44,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2809, 'grad_norm': 18.75294303894043, 'learning_rate': 3.3770361541517685e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5034/5034 [4:40:05<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16805.8714, 'train_samples_per_second': 4.791, 'train_steps_per_second': 0.3, 'train_loss': 2.577739136627282, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Load the tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load the plain text dataset\n",
    "datasets = load_dataset('text', data_files={'train': 'pq_corpus.txt'})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Set up data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-pq-mlm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Run training\n",
    "trainer.train()\n",
    "\n",
    "model_save_path = \"./distilbert-pq-mlm\"\n",
    "\n",
    "# Save the domain-adapted model\n",
    "trainer.save_model(\"./distilbert-pq-mlm\")\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Verify saved files\n",
    "print(f\"Model and tokenizer saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5fd06",
   "metadata": {},
   "source": [
    "281 mins 40.6 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "956e675b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing domain-adapted model:\n",
      "--------------------------------------------------\n",
      "\n",
      "Input: A sudden [MASK] in voltage can cause sensitive equipment to fail.\n",
      "Top predictions:\n",
      "- increase: 0.4798\n",
      "- drop: 0.1674\n",
      "- rise: 0.1308\n",
      "- change: 0.0904\n",
      "- surge: 0.0441\n",
      "\n",
      "Input: The power [MASK] was caused by lightning strike.\n",
      "Top predictions:\n",
      "- problem: 0.1721\n",
      "- loss: 0.1222\n",
      "- quality: 0.0741\n",
      "- interruption: 0.0732\n",
      "- system: 0.0654\n",
      "\n",
      "Input: The distribution [MASK] failed due to overload.\n",
      "Top predictions:\n",
      "- system: 0.5795\n",
      "- feeder: 0.1961\n",
      "- line: 0.0223\n",
      "- systems: 0.0182\n",
      "- lines: 0.0142\n",
      "\n",
      "Input: Multiple [MASK] faults occurred during the thunderstorm.\n",
      "Top predictions:\n",
      "- ground: 0.1447\n",
      "- surge: 0.0926\n",
      "- lightning: 0.0635\n",
      "- fault: 0.0274\n",
      "- transient: 0.0264\n",
      "\n",
      "Input: The [MASK] waveform showed significant distortion.\n",
      "Top predictions:\n",
      "- first: 0.1249\n",
      "- resulting: 0.0728\n",
      "- initial: 0.0675\n",
      "- input: 0.0616\n",
      "- original: 0.0550\n",
      "\n",
      "Input: [MASK] fell on power lines during a storm, affecting multiple feeders; estimated restoration time is 4 hours.\n",
      "Top predictions:\n",
      "- that: 0.1861\n",
      "- they: 0.0442\n",
      "- cables: 0.0318\n",
      "- it: 0.0285\n",
      "- lightning: 0.0214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test loading\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "test_model = AutoModelForMaskedLM.from_pretrained(model_save_path)\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=test_model, tokenizer=test_tokenizer)\n",
    "\n",
    "# Example test sentences relevant to power quality\n",
    "test_sentences = [\n",
    "    \"A sudden [MASK] in voltage can cause sensitive equipment to fail.\",\n",
    "    \"The power [MASK] was caused by lightning strike.\",\n",
    "    \"The distribution [MASK] failed due to overload.\",\n",
    "    \"Multiple [MASK] faults occurred during the thunderstorm.\",\n",
    "    \"The [MASK] waveform showed significant distortion.\", \n",
    "    \"[MASK] fell on power lines during a storm, affecting multiple feeders; estimated restoration time is 4 hours.\"\n",
    "]\n",
    "\n",
    "# Run predictions\n",
    "print(\"\\nTesting domain-adapted model:\")\n",
    "print(\"-\" * 50)\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nInput: {sentence}\")\n",
    "    results = fill_mask(sentence)\n",
    "    print(\"Top predictions:\")\n",
    "    for r in results[:5]:  # Show top 5 predictions\n",
    "        print(f\"- {r['token_str']}: {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d155c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
